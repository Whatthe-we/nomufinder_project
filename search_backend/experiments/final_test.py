import json
import sys
import os
import csv
from dotenv import load_dotenv
from openai import OpenAI
import matplotlib.font_manager as fm
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, accuracy_score

# ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
from classifier import classify_text_with_openai, clean_category_output, categories

# Load API Key
load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
classify_text_with_openai.__globals__["client"] = client

# í•œê¸€ í°íŠ¸ ì„¤ì • (Windowsìš©)
plt.rcParams['font.family'] = 'Malgun Gothic'
plt.rcParams['axes.unicode_minus'] = False     # ë§ˆì´ë„ˆìŠ¤ ë¶€í˜¸ ê¹¨ì§ ë°©ì§€

# === Zero-shot í•¨ìˆ˜ ===
def classify_text_zeroshot(user_input: str) -> str:
    prompt = f"""
ë‹¹ì‹ ì€ ì•„ë˜ ë¬¸ì¥ì„ ë¶„ë¥˜í•˜ëŠ” AIì…ë‹ˆë‹¤.
ë¬¸ì¥ì„ ì•„ë˜ ì¹´í…Œê³ ë¦¬ ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”:

- {', '.join(categories)}

ë¬¸ì¥: {user_input}
ì¹´í…Œê³ ë¦¬:"""
    try:
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3,
            max_tokens=50,
        )
        result = response.choices[0].message.content.strip()
        return clean_category_output(result)
    except Exception as e:
        print("âŒ Zero-shot ì˜¤ë¥˜:", e)
        return "ë¶„ë¥˜ ì‹¤íŒ¨"

# === í‰ê°€ í•¨ìˆ˜ ===
def evaluate_model(name, classify_fn, dataset):
    y_true, y_pred = [], []
    for item in dataset:
        text, label = item["text"], item["label"]
        pred = classify_fn(text)
        y_true.append(label)
        y_pred.append(pred)
        print(f"[{name}] ë¬¸ì¥: {text}\nğŸ‘‰ ì˜ˆì¸¡: {pred} / âœ… ì •ë‹µ: {label}\n")

    acc = accuracy_score(y_true, y_pred)
    report = classification_report(y_true, y_pred, labels=categories, zero_division=0, output_dict=True)
    return {"name": name, "accuracy": acc, "report": report}

# === í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ===
with open("search_backend/experiments/test_set.json", "r", encoding="utf-8") as f:
    test_set = json.load(f)

results = [
    evaluate_model("Few-shot", classify_text_with_openai, test_set),
    evaluate_model("Zero-shot", classify_text_zeroshot, test_set),
]

# === csv ì €ì¥ ===
csv_path = os.path.join("search_backend", "experiments", "final_test_results.csv")
with open(csv_path, "w", newline="", encoding="utf-8") as f:
    writer = csv.writer(f)
    writer.writerow(["Category", "Few-shot F1", "Zero-shot F1"])
    for i, cat in enumerate(categories):
        few_f1 = results[0]["report"][cat]["f1-score"]
        zero_f1 = results[1]["report"][cat]["f1-score"]
        writer.writerow([cat, few_f1, zero_f1])
print(f"ğŸ“„ ê²°ê³¼ CSV ì €ì¥ë¨: {csv_path}")

# === ê·¸ë˜í”„ ì‹œê°í™” ===
plt.figure(figsize=(12, 6))
x = range(len(categories))
for result in results:
    f1_scores = [result["report"][cat]["f1-score"] for cat in categories]
    plt.plot(x, f1_scores, marker="o", label=result["name"])

    # ì  ìœ„ì— í…ìŠ¤íŠ¸ë¡œ f1-score í‘œì‹œ
    for i, score in enumerate(f1_scores):
        plt.text(i, score + 0.01, f"{score:.2f}", ha='center', va='bottom', fontsize=9)

plt.xticks(x, categories, rotation=45, ha="right")
plt.title("Few-shot vs Zero-shot")
plt.xlabel("Category")
plt.ylabel("F1 Score")
plt.legend()
plt.grid(True)
plt.tight_layout()

img_path = os.path.join("search_backend", "experiments", "final_test_f1_comparison.png")
plt.savefig(img_path, dpi=300)
print(f"ğŸ“ ê·¸ë˜í”„ ì €ì¥ë¨: {img_path}")
plt.show()