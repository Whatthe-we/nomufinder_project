# main.py (OpenAI Embedding ì‚¬ìš© ë²„ì „)
import uvicorn
from fastapi import FastAPI, HTTPException
import os
import threading
import time
import traceback
from dotenv import load_dotenv
import pickle
import json

# --- Firebase Admin SDK ---
import firebase_admin
from firebase_admin import credentials, db

# --- LangChain & RAG ê´€ë ¨ ---
from langchain_openai import ChatOpenAI, OpenAIEmbeddings # <<< OpenAIEmbeddings ì¶”ê°€
# from langchain_community.embeddings import HuggingFaceEmbeddings # <<< HuggingFace ì œê±° ë˜ëŠ” ì£¼ì„ ì²˜ë¦¬
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever
# import torch # OpenAI Embedding ì‚¬ìš© ì‹œ torch ì§ì ‘ í•„ìš” ì—†ìŒ (FAISS ë‚´ë¶€ ì˜ì¡´ì„± ê°€ëŠ¥ì„±ì€ ìˆìŒ)

# --- .env íŒŒì¼ ë¡œë“œ ---
load_dotenv()

# === ğŸ”„ í›„ì†ì§ˆë¬¸ ë¶„ë¥˜ê¸° ë° ìƒì„±ê¸° í•¨ìˆ˜ (main.py ë‚´ë¶€ì— í†µí•©) ===
from typing import List, Tuple
from langchain_openai import ChatOpenAI
import os

# === ë¶„ë¥˜ê¸°: ì§ˆë¬¸ì´ ì•„ë‹Œ ì¼ë°˜ ì§„ìˆ ì¸ì§€ í™•ì¸ ===
def classify_need_for_question(user_input: str) -> bool:
    """
    ìœ ì € ì…ë ¥ì´ ì§ˆë¬¸ì´ ì•„ë‹ˆë¼ ë‹¨ìˆœ ì§„ìˆ (ì˜ˆ: "ë‚˜ëŠ” ì‚¬ì—…ì„ í•˜ê³  ìˆì–´ìš”")ì¼ ê²½ìš° True ë°˜í™˜
    """
    question_keywords = ["ë­ì•¼", "ë­”ê°€ìš”", "ì–´ë–»ê²Œ", "ë¬´ì—‡", "ì–¸ì œ", "ì™œ", "ì–¼ë§ˆ", "ê°€ëŠ¥í•´", "ë˜ë‚˜ìš”", "ì¸ê°€ìš”", "?"]
    if user_input.endswith("?"):
        return False
    if any(kw in user_input for kw in question_keywords):
        return False
    return True

# === ìƒì„±ê¸°: í›„ì† ì§ˆë¬¸ ìƒì„± (ë§¥ë½ ê¸°ë°˜) ===
def generate_followup_question(user_input: str, chat_history: List[Tuple[str, str]]) -> str:
    """
    ìœ ì €ì˜ ì§„ìˆ ì„ ê¸°ë°˜ìœ¼ë¡œ, ì–´ë–¤ ì¶”ê°€ ì§ˆë¬¸ì„ í•˜ë©´ ì¢‹ì„ì§€ LLMìœ¼ë¡œ ìƒì„±
    (ì´ì „ ëŒ€í™” ê¸°ë¡ì„ í•¨ê»˜ ê³ ë ¤í•¨)
    """
    openai_key = os.getenv("OPENAI_API_KEY")
    if not openai_key:
        raise ValueError("OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    llm = ChatOpenAI(model_name="gpt-4.1", temperature=0, openai_api_key=openai_key)

    history_prompt = ""
    for user_msg, ai_msg in chat_history[-3:]:  # ìµœê·¼ 3í„´ë§Œ ì‚¬ìš©
        history_prompt += f"ì‚¬ìš©ì: {user_msg}\nì±—ë´‡: {ai_msg}\n"

    prompt = f"""
ë„ˆëŠ” í•œêµ­ ë…¸ë™ë²• ê¸°ë°˜ ì±—ë´‡ì´ì•¼.
ì•„ë˜ëŠ” ì‚¬ìš©ìì™€ì˜ ìµœê·¼ ëŒ€í™” ê¸°ë¡ê³¼ í˜„ì¬ ë°œí™”ì…ë‹ˆë‹¤.

{history_prompt}

í˜„ì¬ ë°œí™”: \"{user_input}\"

ì´ ë°œí™”ëŠ” ì§ˆë¬¸ì´ ì•„ë‹ˆë¼ ì§„ìˆ ì¼ ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤.
ë²•ì  ìƒë‹´ì— í•„ìš”í•œ í•µì‹¬ ì •ë³´ë¥¼ ì–»ê¸° ìœ„í•´ ì´ì–´ì„œ í•´ì•¼ í•  ê°€ì¥ ì ì ˆí•œ í›„ì† ì§ˆë¬¸ í•˜ë‚˜ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ìƒì„±í•˜ì„¸ìš”.

ë‹¨, ë„ˆë¬´ í¬ê´„ì ì¸ ì§ˆë¬¸(ì˜ˆ: \"ë¬´ìŠ¨ ë„ì›€ì´ í•„ìš”í•˜ì„¸ìš”?\")ì€ í”¼í•˜ê³ , êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ ì§ˆë¬¸ì„ ìƒì„±í•˜ì„¸ìš”.
í›„ì† ì§ˆë¬¸:
"""

    try:
        response = llm.invoke(prompt)
        return response.content.strip()
    except Exception as e:
        print(f"!! í›„ì† ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨: {e}")
        return ""

# === ì „ì—­ ë³€ìˆ˜ ===
qa_chain = None
firebase_app = None
is_initialized = False
processed_question_ids = set()  # âœ… ì´ë¯¸ ì²˜ë¦¬í•œ ì§ˆë¬¸ ID ì €ì¥
startup_time = int(time.time())  # âœ… ì„œë²„ ì‹œì‘ ì‹œê°„ ê¸°ë¡

# === ì„¤ì •ê°’ ===
FAISS_INDEX_PATH = "faiss_index_nomu_final"
BM25_DATA_PATH = "split_texts_for_bm25.pkl"
FIREBASE_SERVICE_ACCOUNT_KEY = "serviceAccountKey.json"

# ===> ëª¨ë¸ ì„¤ì • ìˆ˜ì • <===
EMBEDDING_MODEL_NAME = "text-embedding-3-large" # OpenAI ì„ë² ë”© ëª¨ë¸
LLM_MODEL_NAME = "gpt-4.1"                      # âœ… Â ì‚¬ìš©í•  LLM ëª¨ë¸
# =====================

# Firebase ì„¤ì • (í™˜ê²½ ë³€ìˆ˜ì—ì„œ ë¡œë“œ)
FIREBASE_DB_URL = os.getenv("FIREBASE_DATABASE_URL")
if not FIREBASE_DB_URL:
    print("!! [ì¹˜ëª…ì  ì˜¤ë¥˜] .env íŒŒì¼ì—ì„œ FIREBASE_DATABASE_URLì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì•± ì‹¤í–‰ ë¶ˆê°€.")
FIREBASE_QUESTIONS_PATH = "/chat_questions"
FIREBASE_ANSWERS_PATH = "/chat_answers"
QUESTION_LOG_PATH = "chat_log.json"

# Retriever ì„¤ì •ê°’                               # âœ… ì—…ë°ì´íŠ¸
FAISS_K = 4
BM25_K = 4
ENSEMBLE_WEIGHTS = [0.4, 0.6] # BM25=0.4, FAISS=0.6

# === FastAPI ì•± ì¸ìŠ¤í„´ìŠ¤ ===
app = FastAPI()

# === âœ… ì§ˆë¬¸ ì¬ì²˜ë¦¬ ===
PROCESSED_IDS_JSON_PATH = "processed_ids.json"
PROCESSED_IDS_BACKUP_PATH = "processed_ids_backup.json"
MAX_PROCESSED_IDS = 100

def load_processed_ids():
    global processed_question_ids
    if os.path.exists(PROCESSED_IDS_JSON_PATH):
        with open(PROCESSED_IDS_JSON_PATH, "r", encoding="utf-8") as f:
            processed_question_ids = set(json.load(f))
        print(f"âœ… ì²˜ë¦¬ëœ ID {len(processed_question_ids)}ê°œ ë¡œë“œë¨")
    else:
        processed_question_ids = set()

def save_processed_id(question_id):
    global processed_question_ids

    processed_question_ids.add(question_id)
    # ğŸ”„ ìµœëŒ€ ê°œìˆ˜ ì´ˆê³¼ ì‹œ ë°±ì—… + ì •ë¦¬
    if len(processed_question_ids) > MAX_PROCESSED_IDS:
        print(f"ğŸ“¦ ìµœëŒ€ {MAX_PROCESSED_IDS}ê°œ ì´ˆê³¼ë¨ â†’ ë°±ì—… ë° ì •ë¦¬ ìˆ˜í–‰")
        all_ids = sorted(list(processed_question_ids))
        # âœ… ë°±ì—… (ì „ì²´ ì €ì¥)
        with open(PROCESSED_IDS_BACKUP_PATH, "w", encoding="utf-8") as f_backup:
            json.dump(all_ids, f_backup, ensure_ascii=False, indent=2)
        # ìµœì‹  MAX ê°œìˆ˜ë§Œ ìœ ì§€
        processed_question_ids = set(all_ids[-MAX_PROCESSED_IDS:])
    # âœï¸ ìµœì¢… ì €ì¥
    with open(PROCESSED_IDS_JSON_PATH, "w", encoding="utf-8") as f:
        json.dump(list(processed_question_ids), f, ensure_ascii=False, indent=2)

def log_question_answer_pair(question_id, question, answer):
    log_entry = {
        "id": question_id,
        "question": question,
        "answer": answer,
        "timestamp": int(time.time())
    }
    if os.path.exists(QUESTION_LOG_PATH):
        with open(QUESTION_LOG_PATH, "r", encoding="utf-8") as f:
            logs = json.load(f)
    else:
        logs = []
    logs.append(log_entry)
    with open(QUESTION_LOG_PATH, "w", encoding="utf-8") as f:
        json.dump(logs, f, ensure_ascii=False, indent=2)

# === RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” í•¨ìˆ˜ ===
def initialize_rag():
    global qa_chain, is_initialized
    if is_initialized:
        print("[ì •ë³´] RAG íŒŒì´í”„ë¼ì¸ ì´ë¯¸ ì´ˆê¸°í™”ë¨.")
        return

    print("--- RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì‹œì‘ ---")
    vectorstore = None
    split_texts = None
    retriever = None
    llm = None
    embeddings = None # OpenAIEmbeddings ê°ì²´ë¥¼ ë‹´ì„ ë³€ìˆ˜

    try:
        # 1. OpenAI API í‚¤ í™•ì¸ (LLM ë° Embedding ëª¨ë‘ ì‚¬ìš©)
        openai_api_key = os.getenv("OPENAI_API_KEY")
        if not openai_api_key:
            raise ValueError("í™˜ê²½ ë³€ìˆ˜ì—ì„œ OPENAI_API_KEYë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        # ===> 1. ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (OpenAIEmbeddings ì‚¬ìš©) <===
        print(f"ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘ ({EMBEDDING_MODEL_NAME})...")
        embeddings = OpenAIEmbeddings(
            model=EMBEDDING_MODEL_NAME,
            openai_api_key=openai_api_key
            # í•„ìš”ì‹œ ì¶”ê°€ íŒŒë¼ë¯¸í„° ì„¤ì • (e.g., chunk_size for batching)
        )
        print("OpenAI ì„ë² ë”© ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ.")
        # =============================================

        # ===> 2. FAISS ì¸ë±ìŠ¤ ë¡œë“œ <===
        # !! ì¤‘ìš” !! ì´ ì¸ë±ìŠ¤ëŠ” ë°˜ë“œì‹œ OpenAIEmbeddings(text-embedding-ada-002)ë¡œ
        # !!      ìƒì„±ëœ ê²ƒì´ì–´ì•¼ í•©ë‹ˆë‹¤ !!
        print(f"FAISS ì¸ë±ìŠ¤ ë¡œë”© ì¤‘ ({FAISS_INDEX_PATH})...")
        if not os.path.isdir(FAISS_INDEX_PATH):
            raise FileNotFoundError(f"FAISS ì¸ë±ìŠ¤ í´ë”({FAISS_INDEX_PATH})ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. OpenAI ì„ë² ë”©ìœ¼ë¡œ ì¸ë±ìŠ¤ë¥¼ ë‹¤ì‹œ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.")
        try:
            vectorstore = FAISS.load_local(
                FAISS_INDEX_PATH,
                embeddings, # OpenAIEmbeddings ê°ì²´ ì „ë‹¬
                allow_dangerous_deserialization=True
            )
            print(f"FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ ({vectorstore.index.ntotal} ë²¡í„°).")
            print("   âš ï¸ ê²½ê³ : ë¡œë“œëœ FAISS ì¸ë±ìŠ¤ê°€ ë°˜ë“œì‹œ OpenAI ì„ë² ë”© ëª¨ë¸ë¡œ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”!")
        except Exception as faiss_load_e:
             print(f"!! [ì˜¤ë¥˜] FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {faiss_load_e}")
             print("   ì¸ë±ìŠ¤ê°€ ì†ìƒë˜ì—ˆê±°ë‚˜, ì˜ëª»ëœ ì„ë² ë”© ëª¨ë¸ë¡œ ìƒì„±ë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
             print("   OpenAI ì„ë² ë”© ëª¨ë¸ë¡œ FAISS ì¸ë±ìŠ¤ë¥¼ ë‹¤ì‹œ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.")
             raise faiss_load_e # ì´ˆê¸°í™” ì¤‘ë‹¨

        # ==========================

        # 2.5 BM25ìš© ë°ì´í„° ë¡œë“œ (ê¸°ì¡´ê³¼ ë™ì¼)
        print(f"BM25ìš© ë°ì´í„° ë¡œë”© ì¤‘ ({BM25_DATA_PATH})...")
        if not os.path.exists(BM25_DATA_PATH):
             print(f"âš ï¸ ê²½ê³ : BM25 ë°ì´í„° íŒŒì¼({BM25_DATA_PATH}) ì—†ìŒ. FAISSë§Œ ì‚¬ìš©.")
             split_texts = None
        else:
             try:
                 with open(BM25_DATA_PATH, 'rb') as f:
                     split_texts = pickle.load(f)
                 print(f"BM25ìš© ë°ì´í„° ë¡œë“œ ì™„ë£Œ ({len(split_texts)}ê°œ ì²­í¬).")
             except Exception as e_pickle:
                 print(f"!! [ì˜¤ë¥˜] BM25 ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e_pickle}")
                 split_texts = None

        # 3. Retriever ì„¤ì • (í•˜ì´ë¸Œë¦¬ë“œ ì ìš© - ê¸°ì¡´ê³¼ ë™ì¼)
        print("Retriever ì„¤ì • ì¤‘...")
        if vectorstore:
            faiss_retriever = vectorstore.as_retriever(search_kwargs={'k': FAISS_K})
            print(f"- Dense Retriever (FAISS) ì„¤ì • ì™„ë£Œ (k={FAISS_K}).")
            if split_texts:
                try:
                    bm25_retriever = BM25Retriever.from_documents(split_texts)
                    bm25_retriever.k = BM25_K
                    print(f"- Sparse Retriever (BM25) ì„¤ì • ì™„ë£Œ (k={BM25_K}).")
                    ensemble_retriever = EnsembleRetriever(
                        retrievers=[bm25_retriever, faiss_retriever],
                        weights=ENSEMBLE_WEIGHTS
                    )
                    retriever = ensemble_retriever
                    print(f"- Ensemble Retriever ì„¤ì • ì™„ë£Œ (Weights: BM25={ENSEMBLE_WEIGHTS[0]}, FAISS={ENSEMBLE_WEIGHTS[1]}).")
                except Exception as e_ensemble:
                    print(f"!! BM25/Ensemble ì„¤ì • ì‹¤íŒ¨: {e_ensemble}. FAISS Retrieverë§Œ ì‚¬ìš©.")
                    retriever = faiss_retriever
            else:
                print("âš ï¸ BM25 ë°ì´í„° ì—†ì–´ Dense Retriever(FAISS)ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                retriever = faiss_retriever
        else:
            raise ValueError("FAISS VectorStore ë¡œë“œ ì‹¤íŒ¨. Retriever ì„¤ì • ë¶ˆê°€.")

        # 4. LLM ë¡œë“œ (OpenAI - ê¸°ì¡´ê³¼ ë™ì¼)
        print(f"LLM ë¡œë”© ì¤‘ ({LLM_MODEL_NAME})...")
        llm = ChatOpenAI(model_name=LLM_MODEL_NAME, temperature=0, openai_api_key=openai_api_key)
        print("LLM ë¡œë“œ ì™„ë£Œ.")

        # 5. RAG Chain êµ¬ì¶• (ê¸°ì¡´ê³¼ ë™ì¼) # âœ… í”„ë¡¬í”„íŠ¸ ê°œì„ 
        print("RAG Chain êµ¬ì¶• ì¤‘...")
        template = """ë‹¹ì‹ ì€ í•œêµ­ì˜ ë…¸ë¬´ ê·œì • ë° ê´€ë ¨ ë¬¸ì„œì— ê¸°ë°˜í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ìœ ìš©í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì´ë¦„(ëª…ì¹­)ì€ ë…¸ë¬´ë¬´ì…ë‹ˆë‹¤.

        [ë‹µë³€ ë°©ì‹]
        - ì§ˆë¬¸ìì˜ ìƒí™©ì— ê³µê°í•˜ëŠ” ì§§ì€ ì¸ì‚¿ë§ ë˜ëŠ” ê²©ë ¤ ë¬¸ì¥ìœ¼ë¡œ ì‹œì‘í•´ ì£¼ì„¸ìš”. (ì˜ˆ: "ìœ¡ì•„ì™€ ì¼ì„ ë³‘í–‰í•˜ì‹œëŠë¼ ë§ì´ ë°”ì˜ì‹œì£ ?")
        - í•µì‹¬ ì •ë³´ë¥¼ ê°€ì¥ ë¨¼ì €, ê°„ê²°í•œ ë¬¸ì¥ìœ¼ë¡œ ì œì‹œí•´ ì£¼ì„¸ìš”.
        - í•„ìš”í•œ ê²½ìš° ì˜ˆì‹œë‚˜ ì¡°ê±´ì„ ë§ë¶™ì—¬ ì£¼ì„¸ìš”.
        - ë‹µë³€ì˜ ëë§ºìŒì€ "~ì…ë‹ˆë‹¤."ë³´ë‹¤ëŠ” "~ì´ì—ìš”.", "~í•  ìˆ˜ ìˆì–´ìš”.", "~í•˜ì…”ì•¼ í•´ìš”."ì²˜ëŸ¼ **ë¶€ë“œëŸ¬ìš´ ì¢…ê²°ì–´ë¯¸ (~ìš”)** ë¡œ í†µì¼í•´ ì£¼ì„¸ìš”.
        - ë²•ë ¹ì˜ ì˜ë¯¸ëŠ” ì¼ë°˜ ì‚¬ìš©ìê°€ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì‰½ê²Œ í’€ì–´ ì„¤ëª…í•´ ì£¼ì„¸ìš”.
        - ë¬¸ì„œì— ì—†ëŠ” ìˆ˜ì¹˜ í‘œí˜„(ì˜ˆ: 5ì¸ ë¯¸ë§Œ)ì´ë¼ë„ ì˜ë¯¸ê°€ ë™ì¼í•œ ê²½ìš°(ì˜ˆ: 4ëª… ì´í•˜)ë¡œ ë…¼ë¦¬ì ìœ¼ë¡œ í•´ì„í•˜ì—¬ ì„¤ëª…í•´ ì£¼ì„¸ìš”.
        - ì§ˆë¬¸ì— ëŒ€í•œ ëª…í™•í•œ ì¡°í•­ì´ ë¬¸ì„œì— ì—†ë”ë¼ë„, ì§ˆë¬¸ì´ ì¼ë°˜ì ì´ê³  ìƒì‹ì ì´ë¼ë©´ LLMì˜ ì¼ë°˜ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ì‹ ì¤‘íˆ ë³´ì™„í•´ ì£¼ì„¸ìš”.
        - ë„ˆë¬´ ê¸´ ë¬¸ë‹¨ ì—†ì´, **ê°€ë…ì„±ì„ ë†’ì´ê¸° ìœ„í•´ 2~3ë¬¸ì¥ ë‹¨ìœ„ë¡œ ì¤„ì„ ë°”ê¿”** ì£¼ì„¸ìš”.
        - ê´€ë ¨ ì¡°í•­ì´ ìˆëŠ” ê²½ìš°ì—ëŠ” íŒë‹¨ì„ ë¯¸ë£¨ì§€ ë§ê³ , í•´ë‹¹ ì¡°ë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ ëª…í™•íˆ ë‹µë³€í•´ ì£¼ì„¸ìš”.
        - í˜•ì‚¬ì²˜ë²ŒÂ·ë²Œê¸ˆÂ·ê³¼íƒœë£Œ ì¡°í•­ì´ ë¬¸ì„œì— ëª…ì‹œë˜ì–´ ìˆë‹¤ë©´ ë°˜ë“œì‹œ í¬í•¨í•´ ì£¼ì„¸ìš”.

        [ì‹ ë¢°ì„± ë° ìµœì‹  ë²•ë ¹ ë°˜ì˜ ì¡°ê±´]
        - 2025ë…„ ê¸°ì¤€ ìµœì‹  ê°œì • ë²•ë ¹ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”. (ì˜ˆ: ìœ¡ì•„íœ´ì§ 1ë…„ 6ê°œì›”)
        - ìœ„ë°˜ ì‹œ ì œì¬ ìˆ˜ìœ„ëŠ” "í–‰ì •ì²˜ë¶„", "ê³¼íƒœë£Œ", "í˜•ì‚¬ì²˜ë²Œ" ì¤‘ êµ¬ì²´ì ìœ¼ë¡œ êµ¬ë¶„í•´ ì£¼ì„¸ìš”.
        - í•´ë‹¹ ì œì¬ ì¡°í•­ì´ ë¬¸ì„œì— ì—†ê±°ë‚˜ ëª…í™•í•˜ì§€ ì•Šì€ ê²½ìš°ì—ëŠ” "ë¬¸ì„œë§Œìœ¼ë¡œëŠ” íŒë‹¨ì´ ì–´ë ¤ì›Œìš”."ë¼ê³  ì•ˆë‚´í•´ ì£¼ì„¸ìš”.
        - ë²•ë ¹ì— ëª…ì‹œëœ ê²½ìš°ì—ë§Œ í˜•ì‚¬ì²˜ë²Œì´ë‚˜ ê³¼íƒœë£Œ ë‚´ìš©ì„ ì–¸ê¸‰í•´ ì£¼ì„¸ìš”.
        - í•´ì„ì´ ë¶„ë¶„í•˜ê±°ë‚˜ ìŸì ì´ ìˆëŠ” ê²½ìš°ì—ë„ "ë¬¸ì„œë§Œìœ¼ë¡œëŠ” íŒë‹¨ì´ ì–´ë ¤ì›Œìš”."ë¼ê³  ì•ˆë‚´í•´ ì£¼ì„¸ìš”.
        - ë‹¨ì •ì ì´ê±°ë‚˜ ê¸°ê³„ì ì¸ í‘œí˜„ì€ í”¼í•˜ê³ , ë¶€ë“œëŸ½ê³  ì±…ì„ê° ìˆëŠ” ë§íˆ¬ë¡œ ë§ˆë¬´ë¦¬í•´ ì£¼ì„¸ìš”.
        - ì¼ë°˜ì ì¸ ë²•ë¥  ì§€ì‹ì„ í™œìš©í•˜ë˜, ë¬¸ì„œì˜ ë‚´ìš©ì„ ìš°ì„ í•˜ì—¬ ì‹ ì¤‘í•˜ê²Œ ì‘ì„±í•´ ì£¼ì„¸ìš”.
        - ì§ˆë¬¸ì´ ì¼ë°˜ì ì´ê³  ì»¨í…ìŠ¤íŠ¸ì— ê´€ë ¨ ì¡°í•­ì´ ì—†ì„ ê²½ìš°, LLMì˜ ì¼ë°˜ì ì¸ ë²•ë¥  ì§€ì‹ì„ ê¸°ë°˜ìœ¼ë¡œ ë³´ì™„í•˜ë˜, ë¬¸ì„œë³´ë‹¤ ìš°ì„ í•˜ì§€ ì•Šë„ë¡ ì£¼ì˜í•˜ì‹­ì‹œì˜¤.
        - íŠ¹íˆ ë…¸ë™ ê´€ë ¨ ë²•ë ¹ì—ì„œ ì œì¬ ìˆ˜ìœ„ëŠ” í•µì‹¬ì´ë¯€ë¡œ ëˆ„ë½ ì—†ì´ ì„œìˆ í•˜ì‹­ì‹œì˜¤.

        [ì–¸ì–´ ìŠ¤íƒ€ì¼]
        - ê¸°ê³„ì ì¸ í‘œí˜„ì€ í”¼í•˜ê³ , ìì—°ìŠ¤ëŸ¬ìš´ ì„¤ëª…í˜• ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”.
        - ë¬¸ì¥ ëì€ "~ì…ë‹ˆë‹¤."ë³´ë‹¤ëŠ” "~ì´ì—ìš”", "~í•˜ì…”ì•¼ í•´ìš”", "~í•˜ì‹¤ ìˆ˜ ìˆì–´ìš”" ë“±ì˜ ë¶€ë“œëŸ¬ìš´ ë§íˆ¬ë¥¼ ì‚¬ìš©í•´ ì£¼ì„¸ìš”.
        - ì˜ˆì‹œ í‘œí˜„ì€ ë§¤ë²ˆ "ì˜ˆë¥¼ ë“¤ì–´"ë³´ë‹¤ëŠ” "ì˜ˆì»¨ëŒ€", "ë³´í†µ ì´ëŸ° ê²½ìš°", "ì˜ˆì‹œë¥¼ ë“¤ë©´" ë“±ìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ë‹¤ì–‘í™”í•´ ì£¼ì„¸ìš”.
        - í•µì‹¬ ì •ë³´ â†’ ì˜ˆì‹œ â†’ ì¡°ê±´/ì˜ˆì™¸ â†’ ë§ˆë¬´ë¦¬ ì•ˆë‚´ ìˆœìœ¼ë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”.
        - ë„ˆë¬´ ê¸´ ë¬¸ë‹¨ ì—†ì´ ìì—°ìŠ¤ëŸ½ê²Œ ë¬¸ë‹¨ì„ ë‚˜ëˆ„ì–´ ì£¼ì„¸ìš”.
        - í•„ìš”í•œ ê²½ìš° "ë” ìì„¸í•œ ë‚´ìš©ì€ ë…¸ë¬´ì‚¬ ìƒë‹´ì´ë‚˜ íšŒì‚¬ ë‚´ê·œ í™•ì¸ì´ í•„ìš”í•´ìš”."ì™€ ê°™ì´ ì•ˆë‚´í•´ ì£¼ì„¸ìš”.
        - ë§ˆí¬ë‹¤ìš´ êµµì€ ê¸€ì ë“± íŠ¹ìˆ˜ í¬ë§·ì€ ì‚¬ìš©í•˜ì§€ ë§ê³ , ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œ ì¶œë ¥í•´ ì£¼ì„¸ìš”.

        [ì •ë¦¬]
        - ì§ˆë¬¸ìê°€ ë¶ˆì•ˆí•˜ê±°ë‚˜ í˜¼ë€ìŠ¤ëŸ¬ìš¸ ìˆ˜ ìˆëŠ” ìƒí™©ì¼ ê²½ìš°, ì§§ê³  ìì—°ìŠ¤ëŸ¬ìš´ **ê³µê° í‘œí˜„**ìœ¼ë¡œ ë‹µë³€ì„ ì‹œì‘í•´ ì£¼ì„¸ìš”. (ì˜ˆ: â€œì´ëŸ° ìƒí™©ì—ì„œëŠ” í˜¼ë€ìŠ¤ëŸ¬ìš°ì‹¤ ìˆ˜ ìˆì–´ìš”.â€)
        - ë‹¨ìˆœí•œ ì‚¬ì‹¤ í™•ì¸ ì§ˆë¬¸ì¼ ê²½ìš°ì—ëŠ” ê³µê° í‘œí˜„ ì—†ì´ ë°”ë¡œ **í•µì‹¬ ì •ë³´**ë¶€í„° ì œì‹œí•´ ì£¼ì„¸ìš”.
        - ì „ì²´ ë‹µë³€ì€ ë‹¤ìŒ ìˆœì„œë¡œ êµ¬ì„±í•˜ì„¸ìš”:
          1. (ì¡°ê±´ë¶€) ê³µê° í‘œí˜„ 1ë¬¸ì¥
          2. í•µì‹¬ ì •ë³´ ìš”ì•½
          3. ì˜ˆì‹œ ë˜ëŠ” ì¼ë°˜ì ì¸ ì ìš© ì¡°ê±´
          4. ì œí•œì‚¬í•­ ë˜ëŠ” ìœ ì˜ì‚¬í•­
        - ë„ˆë¬´ í˜•ì‹ì ì´ì§€ ì•Šê²Œ, ëŒ€í™”í•˜ë“¯ í¸ì•ˆí•˜ê³  ë¯¿ì„ ìˆ˜ ìˆëŠ” ë§íˆ¬ë¡œ ì„¤ëª…í•´ ì£¼ì„¸ìš”."

            ì»¨í…ìŠ¤íŠ¸:
            {context}

            ì§ˆë¬¸: {question}

            ë‹µë³€ (í•œêµ­ì–´):"""

        QA_CHAIN_PROMPT = PromptTemplate(
            input_variables=["context", "question"],
            template=template
        )

        if not retriever: raise ValueError("Retriever ê°ì²´ê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        if not llm: raise ValueError("LLM ê°ì²´ê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        global qa_chain # ì „ì—­ ë³€ìˆ˜ í• ë‹¹ ëª…ì‹œ
        qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=retriever,
            return_source_documents=True,
            chain_type_kwargs={
                    "prompt": QA_CHAIN_PROMPT
                },
                input_key="question"  # âœ… ì¶”ê°€
            )
        is_initialized = True
        print("[ì„±ê³µ] RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ.")

    except Exception as e:
        print(f"!! [ì˜¤ë¥˜] RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        traceback.print_exc()
        qa_chain = None
        is_initialized = False

# --- âœ… Firebase ë¦¬ìŠ¤ë„ˆ ì½œë°± í•¨ìˆ˜ ---
def firebase_listener_callback(event):
    print(f"\n--- Firebase ì´ë²¤íŠ¸ ê°ì§€ ---")
    print(f"  ì´ë²¤íŠ¸ íƒ€ì…: {event.event_type}, ê²½ë¡œ: {event.path}")

    # ì—¬ëŸ¬ ì§ˆë¬¸ (ì´ˆê¸° ì „ì²´ ë¡œë”© ì‹œ ë°œìƒ)
    if event.path == "/" and isinstance(event.data, dict):
        print("ğŸ“¦ ë£¨íŠ¸ì—ì„œ ì—¬ëŸ¬ ì§ˆë¬¸ì„ ë™ì‹œì— ê°ì§€í•¨.")

        # âœ… ìµœê·¼ 5ê°œ IDë§Œ ì¶”ì¶œ (timestamp ê¸°ì¤€)
        sorted_items = sorted(
            event.data.items(),
            key=lambda item: item[1].get("timestamp", 0),
            reverse=True
        )
        recent_items = sorted_items[:5]  # ê°€ì¥ ìµœì‹  5ê°œë§Œ

        for question_id, question_data in recent_items:
            if isinstance(question_data, dict) and ("query" in question_data or "chat_history" in question_data):
                handle_single_question(question_id, question_data)
        return

    # ë‹¨ì¼ ì§ˆë¬¸ ê°ì§€
    if event.event_type == 'put' and event.data:
        if isinstance(event.data, dict) and ("query" in event.data or "chat_history" in event.data):
            question_id = event.path.strip("/").split("/")[-1]
            handle_single_question(question_id, event.data)
            return

    print(f"âš ï¸ ë¬´ì‹œëœ ì´ë²¤íŠ¸ ë˜ëŠ” query/chat_history ì—†ìŒ: {event.path}, ë°ì´í„°: {event.data}")

def handle_single_question(question_id, question_data):
    global qa_chain, processed_question_ids

    question_time = question_data.get("timestamp", 0)
    if question_id in processed_question_ids:
        print(f"âš ï¸ ì´ë¯¸ ì²˜ë¦¬ëœ ì§ˆë¬¸: {question_id} â†’ ìŠ¤í‚µí•¨")
        return

    # ê¸°ë³¸ query ë˜ëŠ” chat_history
    query = question_data.get("query", "").strip()
    chat_history = question_data.get("chat_history", [])

    if not query and not chat_history:
        print(f"âš ï¸ ì§ˆë¬¸ ë‚´ìš©ì´ ë¹„ì–´ìˆìŒ: {question_id}")
        return

    try:
        print(f"  âœ… ìœ íš¨í•œ ì§ˆë¬¸ ê°ì§€ë¨ (ID: {question_id})")
        print("  RAG ë‹µë³€ ìƒì„± ì‹œë„...")
        start_time = time.time()

        followup = None

        # ğŸ”„ chat_history ë¬¸ë§¥ ê¸°ë°˜ ë‹µë³€
        if isinstance(chat_history, list) and chat_history:
            print("  ğŸ” chat_history ê¸°ë°˜ ë¬¸ë§¥ ì²˜ë¦¬ ì¤‘...")
            context_string = "\n".join(
                f"{item['role'].capitalize()}: {item['content']}"
                for item in chat_history if 'role' in item and 'content' in item
            )
            final_question = chat_history[-1]['content']  # ë§ˆì§€ë§‰ ì‚¬ìš©ì ë°œí™”ë§Œ ì¶”ì¶œ

            # í›„ì†ì§ˆë¬¸ í•„ìš” ì—¬ë¶€ íŒë‹¨
            need_followup = classify_need_for_question(final_question)
            chat_history_tuples = [(msg['content'], "") for msg in chat_history if msg['role'] == 'user']
            followup = generate_followup_question(final_question, chat_history_tuples) if need_followup else ""

            full_question = f"""
ë‹¤ìŒì€ ì‚¬ìš©ìì™€ì˜ ìµœê·¼ ëŒ€í™”ì…ë‹ˆë‹¤.
{context_string}

[ì§ˆë¬¸]
{final_question}
""".strip()

        else:
            # chat_historyê°€ ì—†ì„ ë•ŒëŠ” queryë¥¼ ë°”ë¡œ ì§ˆë¬¸ìœ¼ë¡œ ì‚¬ìš©
            final_question = query or "ì§ˆë¬¸ì´ ì—†ìŠµë‹ˆë‹¤."
            full_question = final_question

            # âœ… chat_historyê°€ ì—†ì„ ë•Œë„ í›„ì†ì§ˆë¬¸ íŒë‹¨ ë° ìƒì„± ì¶”ê°€
            if query:
                need_followup = classify_need_for_question(query)
                if need_followup:
                    followup = generate_followup_question(query, [(query, "")])

        # âœ… ì‹¤ì œ GPT í˜¸ì¶œ
        result = qa_chain.invoke({
            "context": "",
            "question": full_question,
        })

        end_time = time.time()
        answer = result.get("result", "ì˜¤ë¥˜: ë‹µë³€ ìƒì„± ì‹¤íŒ¨")
        print(f"  ë‹µë³€ ìƒì„± ì™„ë£Œ ({end_time - start_time:.2f}ì´ˆ). ë‹µë³€: {answer[:50]}...")

        source_info = []
        source_documents = result.get("source_documents", [])
        if source_documents:
            source_info = [
                {"content": doc.page_content[:100]+"...", "metadata": {k: str(v)[:50] for k, v in doc.metadata.items()}}
                for doc in source_documents[:2]
            ]

        db.reference(f"{FIREBASE_ANSWERS_PATH}/{question_id}").set({
            "answer": answer,
            "timestamp": int(time.time()),
            "sources": source_info,
            "followup_question": followup or "",  # âœ… í›„ì† ì§ˆë¬¸ í•¨ê»˜ ì €ì¥
        })
        print(f"  Firebase '{FIREBASE_ANSWERS_PATH}/{question_id}' ê²½ë¡œì— ë‹µë³€ ì €ì¥ ì™„ë£Œ.")

        # âœ… ê¸°ë¡ ë° ë¡œê·¸ ì €ì¥
        save_processed_id(question_id)
        log_question_answer_pair(question_id, query or final_question, answer)

    except Exception as e:
        print(f"!! RAG ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        traceback.print_exc()
        try:
            db.reference(f"{FIREBASE_ANSWERS_PATH}/{question_id}").set({
                "error": f"{type(e).__name__}: {str(e)[:200]}",
                "timestamp": int(time.time())
            })
        except Exception as db_err:
            print(f"!! Firebase ì˜¤ë¥˜ ê¸°ë¡ ì‹¤íŒ¨: {db_err}")

# === FastAPI ì‹œì‘ ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ===
@app.on_event("startup")
async def startup_event_handler():
    global firebase_app
    print("--- FastAPI Application Startup ---")
    load_processed_ids()  # ğŸ”¥ ì´ ì¤„ ì¶”ê°€

    initialize_rag() # RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ë¨¼ì € ì‹œë„

    if not FIREBASE_DB_URL: print("!! [ì¹˜ëª…ì  ì˜¤ë¥˜] FIREBASE_DATABASE_URL í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Firebase ë¦¬ìŠ¤ë„ˆ ì‹œì‘ ë¶ˆê°€."); return
    if not os.path.exists(FIREBASE_SERVICE_ACCOUNT_KEY): print(f"!! [ì¹˜ëª…ì  ì˜¤ë¥˜] Firebase ì„œë¹„ìŠ¤ ê³„ì • í‚¤ íŒŒì¼({FIREBASE_SERVICE_ACCOUNT_KEY})ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Firebase ë¦¬ìŠ¤ë„ˆ ì‹œì‘ ë¶ˆê°€."); return

    try:
        cred = credentials.Certificate(FIREBASE_SERVICE_ACCOUNT_KEY)
        if not firebase_admin._apps:
             firebase_app = firebase_admin.initialize_app(cred, {'databaseURL': FIREBASE_DB_URL})
             print("[ì„±ê³µ] Firebase Admin SDK ì´ˆê¸°í™” ì™„ë£Œ.")
        else:
             firebase_app = firebase_admin.get_app()
             print("[ì •ë³´] Firebase Admin SDK ì´ë¯¸ ì´ˆê¸°í™”ë¨.")

        print(f"Firebase ê²½ë¡œ '{FIREBASE_QUESTIONS_PATH}' ë¦¬ìŠ¤ë‹ ì‹œì‘...")
        listener_thread = threading.Thread(
            target=db.reference(FIREBASE_QUESTIONS_PATH).listen,
            args=(firebase_listener_callback,),
            daemon=True
        )
        listener_thread.start()
        print("[ì •ë³´] Firebase ë¦¬ìŠ¤ë„ˆ ìŠ¤ë ˆë“œ ì‹œì‘ë¨.")

    except Exception as e:
        print(f"!! [ì˜¤ë¥˜] Firebase ì´ˆê¸°í™” ë˜ëŠ” ë¦¬ìŠ¤ë„ˆ ì‹œì‘ ì‹¤íŒ¨: {e}")
        traceback.print_exc()

# === ê¸°ë³¸ ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸ ===
@app.get("/")
async def read_root():
    return {"message": "Nomu RAG Backend is running!", "initialized": is_initialized}

# === ë¡œì»¬ ì‹¤í–‰ìš© ì½”ë“œ ===
if __name__ == "__main__":
    print("--- ë¡œì»¬ ì„œë²„ ì‹œì‘ (uvicorn) ---")
    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=True)